{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# I-MOTION Model ",
   "id": "d55c8b69028c2f30"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T15:28:15.571192Z",
     "start_time": "2024-06-10T15:28:14.021196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "window_size = 600\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")"
   ],
   "id": "3a1093f4cb8ad685",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available. Using CPU.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Data",
   "id": "22bcf9d0d02583f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T15:29:01.384690Z",
     "start_time": "2024-06-10T15:28:22.856159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data_x = np.loadtxt(\"./train/raw_data/Acc_x.txt\", delimiter=\" \")\n",
    "train_data_y = np.loadtxt(\"./train/raw_data/Acc_y.txt\", delimiter=\" \")\n",
    "train_data_z = np.loadtxt(\"./train/raw_data/Acc_z.txt\", delimiter=\" \")\n",
    "train_label = np.loadtxt(\"./train/train_label.txt\", delimiter=\" \")\n",
    "train_order = np.loadtxt(\"./train/train_order.txt\", dtype=float)\n",
    "acc_data = np.dstack((train_data_x, train_data_y, train_data_z, train_label))\n",
    "acc_data = acc_data[train_order.astype(np.int_).flatten() - 1, :, :]\n",
    "window_data = np.empty((acc_data.shape[0] * (acc_data.shape[1]//window_size), window_size, 3), dtype=np.float32)\n",
    "windowed_labels = np.empty((acc_data.shape[0] * (acc_data.shape[1]//window_size), 1), dtype=np.float32)\n",
    "\n",
    "window_index = 0\n",
    "for frame_index in range(acc_data.shape[0]):\n",
    "    \n",
    "    frame_features = acc_data[frame_index, :, :3]\n",
    "    frame_labels = acc_data[frame_index, :, 3]\n",
    "    \n",
    "    for start in range(0, acc_data.shape[1], window_size):\n",
    "        end = start + window_size\n",
    "        window_data[window_index] = frame_features[start:end, :]\n",
    "        # Assuming all samples in a window have the same label, take the label of the first sample\n",
    "        windowed_labels[window_index] = frame_labels[start]\n",
    "        window_index += 1\n",
    "        \n",
    "test_data_x = np.loadtxt(\"./test/test_raw_data/Acc_x.txt\", delimiter=\" \")\n",
    "test_data_y = np.loadtxt(\"./test/test_raw_data/Acc_y.txt\", delimiter=\" \")\n",
    "test_data_z = np.loadtxt(\"./test/test_raw_data/Acc_z.txt\", delimiter=\" \")\n",
    "test_label = np.loadtxt(\"./test/test_label.txt\", delimiter=\" \")\n",
    "test_order = np.loadtxt(\"./test/test_order.txt\", dtype=float)\n",
    "acc_data = np.dstack((test_data_x, test_data_y, test_data_z, test_label))\n",
    "acc_data = acc_data[test_order.astype(np.int_).flatten() - 1, :, :]\n",
    "test_window_data = np.empty((acc_data.shape[0] * (acc_data.shape[1]//window_size), window_size, 3), dtype=np.float32)\n",
    "test_windowed_labels = np.empty((acc_data.shape[0] * (acc_data.shape[1]//window_size), 1), dtype=np.float32)\n",
    "\n",
    "window_index = 0\n",
    "for frame_index in range(acc_data.shape[0]):\n",
    "    \n",
    "    frame_features = acc_data[frame_index, :, :3]\n",
    "    frame_labels = acc_data[frame_index, :, 3]\n",
    "    \n",
    "    for start in range(0, acc_data.shape[1], window_size):\n",
    "        end = start + window_size\n",
    "        test_window_data[window_index] = frame_features[start:end, :]\n",
    "        # Assuming all samples in a window have the same label, take the label of the first sample\n",
    "        test_windowed_labels[window_index] = frame_labels[start]\n",
    "        window_index += 1\n",
    "\n",
    "# Verify the shapes\n",
    "print(\"Train Features shape:\", window_data.shape)\n",
    "print(\"Train Labels shape:\", windowed_labels.shape)\n",
    "print(\"Test Features shape:\", test_window_data.shape)\n",
    "print(\"Test Labels shape:\", test_windowed_labels.shape)\n",
    "    "
   ],
   "id": "69106f9c06a7d448",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Features shape: (163100, 600, 3)\n",
      "Train Labels shape: (163100, 1)\n",
      "Test Features shape: (56980, 600, 3)\n",
      "Test Labels shape: (56980, 1)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Helper Function",
   "id": "98082d55a584d3f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T15:29:35.282835Z",
     "start_time": "2024-06-10T15:29:35.276521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_training_losses(train_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses)\n",
    "    plt.title('Training Loss Over Epochs', fontsize=16)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calculate_padding(kernel_size):\n",
    "    return (kernel_size - 1) // 2\n",
    "\n",
    "def calculate_output_length(input_length, kernel_size, stride, padding):\n",
    "    return ((input_length + 2 * padding - kernel_size) // stride) + 1\n",
    "\n",
    "def calculate_pooling_padding(window_size, stride, input_size, output_size):\n",
    "    return ((output_size - 1) * stride - input_size + window_size) // 2"
   ],
   "id": "ab1afb017abc5998",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "104355b2c124d563"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T15:32:46.752799Z",
     "start_time": "2024-06-10T15:32:08.091672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from scipy.signal import butter, filtfilt\n",
    "def preprocess_accelerometer_data(accel_data, m=5, cutoff=0.001, fs=100, order=5):\n",
    "    # Initialize gravity and linear acceleration\n",
    "    \n",
    "    # Step 1: Remove Gravity\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    linear_acceleration = filtfilt(b, a, accel_data, axis=0)\n",
    "    \n",
    "    # Step 2: Smooth Data\n",
    "    # Create a moving average (MA) filter\n",
    "    window = np.ones(m) / m\n",
    "    # Apply the moving average filter to each column\n",
    "    smoothed_data = np.zeros_like(linear_acceleration)\n",
    "    for i in range(linear_acceleration.shape[1]):\n",
    "        smoothed_data[:, i] = np.convolve(linear_acceleration[:, i], window, mode='same')\n",
    "\n",
    "    # Step 3: Calculate Magnitude\n",
    "    magnitudes = np.sqrt(np.sum(smoothed_data**2, axis=1))\n",
    "    return magnitudes\n",
    "\n",
    "\n",
    "# window_data size is 163100 x 600 x 3.\n",
    "train_processed_data = np.array([preprocess_accelerometer_data(window_data[i, :, :]) for i in range(window_data.shape[0])])\n",
    "test_processed_data = np.array([preprocess_accelerometer_data(test_window_data[i, :, :]) for i in range(test_window_data.shape[0])])\n",
    "print(train_processed_data.shape)  # Output the shape of the processed data\n",
    "print(test_processed_data.shape)"
   ],
   "id": "6c182e7b1d7a5ba5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(163100, 600)\n",
      "(56980, 600)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T15:37:05.769539Z",
     "start_time": "2024-06-10T15:37:05.762353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "for k in range(1, K + 1):  # k is 1-indexed in the mathematical formula\n",
    "        if k <= m // 2:\n",
    "            # Early data points: smaller window size that grows\n",
    "            smoothed_data[k - 1, :] = np.sum(linear_acceleration[:2 * k - 1, :], axis=0) / (2 * k - 1)\n",
    "        elif k > K - m // 2:\n",
    "            # Late data points: smaller window size that shrinks\n",
    "            smoothed_data[k - 1, :] = np.sum(linear_acceleration[2 * k - K - 1:, :]) / (2 * (K - k) + 1)\n",
    "        else:\n",
    "            # Middle data points: fixed window size\n",
    "            smoothed_data[k - 1, :] = np.sum(linear_acceleration[k - m // 2 - 1 : k + m // 2, :]) / m\n",
    "\"\"\""
   ],
   "id": "cf0f406ce8c9eae5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor k in range(1, K + 1):  # k is 1-indexed in the mathematical formula\\n        if k <= m // 2:\\n            # Early data points: smaller window size that grows\\n            smoothed_data[k - 1, :] = np.sum(linear_acceleration[:2 * k - 1, :], axis=0) / (2 * k - 1)\\n        elif k > K - m // 2:\\n            # Late data points: smaller window size that shrinks\\n            smoothed_data[k - 1, :] = np.sum(linear_acceleration[2 * k - K - 1:, :]) / (2 * (K - k) + 1)\\n        else:\\n            # Middle data points: fixed window size\\n            smoothed_data[k - 1, :] = np.sum(linear_acceleration[k - m // 2 - 1 : k + m // 2, :]) / m\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Construction",
   "id": "490b76b9e6eefb4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T21:11:19.868161Z",
     "start_time": "2024-06-10T21:11:19.860350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IMOTION_CNN(nn.Module):\n",
    "    def __init__(self, in_feature=600, in_channel=1, out_feature=7, pool_window_size=4, pool_stride_size=2, \n",
    "                 pool_padding=1, conv_filter_list=[32, 64, 64, 64, 64, 64], conv_kernel_list=[15, 10, 10, 5, 5, 5], conv_stride=1, full_connection_size=200):\n",
    "        super(IMOTION_CNN, self).__init__()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=pool_window_size, stride=pool_stride_size, padding=pool_padding)\n",
    "        assert len(conv_filter_list) == len(conv_kernel_list)\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=in_channel if i == 0 else conv_filter_list[i-1], \n",
    "                      out_channels=conv_filter_list[i], \n",
    "                      kernel_size=conv_kernel_list[i], stride=conv_stride, padding=calculate_padding(conv_kernel_list[i]))\n",
    "            for i in range(len(conv_filter_list))\n",
    "        ])\n",
    "        self.final_matrix_size = conv_filter_list[-1] * (in_feature // (2 ** (len(conv_filter_list))))\n",
    "        self.fc1 = nn.Linear(self.final_matrix_size, full_connection_size) \n",
    "        self.fc2 = nn.Linear(full_connection_size, out_feature)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv in self.conv_layers:\n",
    "            print(\"x: \" + str(x.shape))\n",
    "            x = self.pool(torch.relu(conv(x)))\n",
    "        x = x.view(x.shape[0], -1)  # Flattening the tensor\n",
    "        print(\"x: \" + str(x.shape))\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "id": "c473c5560a7c999d",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset",
   "id": "e45a94f47793432e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T21:11:20.295526Z",
     "start_time": "2024-06-10T21:11:20.290562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a custom Dataset\n",
    "class AccDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)  # Convert features to PyTorch tensors\n",
    "        self.labels = torch.tensor(labels, dtype=torch.int32)       # Convert labels to PyTorch tensors\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "def prepare_data_loaders(train_features, train_labels, test_features, test_labels, batch_size=10):\n",
    "\n",
    "    train_dataset = AccDataset(train_features, train_labels)\n",
    "    test_dataset = AccDataset(test_features, test_labels)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ],
   "id": "7d2807cea662af84",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training & Evaluation",
   "id": "33cd9c130671e96c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T21:11:21.032258Z",
     "start_time": "2024-06-10T21:11:21.027444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, epochs=100, device='cpu'):\n",
    "    model.train()  # Set the model to training mode\n",
    "    loss_values = []  # Initialize a list to store the average loss per epoch\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0  # Track total loss for each epoch\n",
    "\n",
    "        for X, labels in train_loader:\n",
    "            # Move data to the specified device\n",
    "            X, labels = X.to(device), labels.to(device)\n",
    "            X = X.unsqueeze(1)\n",
    "            print(X.shape)\n",
    "            optimizer.zero_grad()  # Clear gradients for the next train step\n",
    "            output = model(X)  # Forward pass\n",
    "            loss = criterion(output, labels)  # Compute the loss\n",
    "            loss.backward()  # Backward pass to compute gradients\n",
    "            optimizer.step()  # Update model parameters\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate the loss\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)  # Calculate average loss\n",
    "        loss_values.append(avg_loss)  # Append average loss to list\n",
    "\n",
    "        # Print the average loss for the current epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "    return loss_values\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, device='cpu'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    true_labels = []  # List to store actual labels\n",
    "    predictions = []  # List to store model predictions\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for X, labels in test_loader:\n",
    "            # Move data to the specified device\n",
    "            X, labels = X.to(device), labels.to(device)\n",
    "\n",
    "            output = model(X)  # Forward pass\n",
    "            _, predicted = torch.max(output.data, 1)  # Get the index of the max log-probability\n",
    "\n",
    "            true_labels += labels.tolist()  # Append actual labels\n",
    "            predictions += predicted.tolist()  # Append predicted labels\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='weighted')\n",
    "    recall = recall_score(true_labels, predictions, average='weighted')\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ],
   "id": "5d6a8d41cfa1c776",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main Script",
   "id": "855dcd082ae8e1f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T21:11:22.259850Z",
     "start_time": "2024-06-10T21:11:21.771860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function to train and evaluate Graph Convolutional Network (GCN) models\n",
    "    with different graph normalization techniques, visualize training metrics, and perform\n",
    "    embedding analysis through PCA.\n",
    "\n",
    "    Assumes the presence of a GCN model class, data loader preparation functions, and\n",
    "    various normalization technique functions defined outside this script.\n",
    "    \"\"\"\n",
    "    # Configuration parameters\n",
    "    num_epochs = 200  # Number of training epochs\n",
    "    # Dictionary mapping normalization technique names to their corresponding functions\n",
    "\n",
    "    # Lists for storing evaluation metrics and model information\n",
    "    metric_values = [[] for _ in range(4)]  # Lists to store Accuracy, Precision, Recall, F1 Score\n",
    "    train_losses = []  # Training loss values for each normalization technique\n",
    "\n",
    "    # Set the computation device (GPU or CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(f\"\\nTraining model\")\n",
    "    # Prepare data loaders\n",
    "    train_loader, test_loader = prepare_data_loaders(train_processed_data, windowed_labels, test_processed_data, test_windowed_labels, batch_size=50)\n",
    "    print(f\"DataLoader batch size: {train_loader.batch_size}\")\n",
    "\n",
    "    # Initialize the GCN model, optimizer, and loss criterion\n",
    "    model = IMOTION_CNN().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train the model\n",
    "    train_losses = train_model(model, train_loader, optimizer, criterion, epochs=num_epochs, device=device)\n",
    "\n",
    "    # Evaluate the model's performance\n",
    "    accuracy, precision, recall, f1 = evaluate_model(model, test_loader, device=device)\n",
    "    # Store the evaluation metrics\n",
    "    metric_values[0].append(accuracy)\n",
    "    metric_values[1].append(precision)\n",
    "    metric_values[2].append(recall)\n",
    "    metric_values[3].append(f1)\n",
    "\n",
    "    # Output the evaluation results\n",
    "    print(f\"Results  - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Visualization of training losses and evaluation metrics for each normalization technique\n",
    "    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    plot_training_losses(train_losses)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "78d42560882cae31",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model\n",
      "DataLoader batch size: 50\n",
      "torch.Size([50, 1, 600])\n",
      "x: torch.Size([50, 1, 600])\n",
      "x: torch.Size([50, 32, 300])\n",
      "x: torch.Size([50, 64, 149])\n",
      "x: torch.Size([50, 64, 74])\n",
      "x: torch.Size([50, 64, 37])\n",
      "x: torch.Size([50, 64, 18])\n",
      "x: torch.Size([50, 64, 9])\n",
      "x: torch.Size([50, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (50x256 and 576x200)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[54], line 50\u001B[0m\n\u001B[1;32m     47\u001B[0m     plot_training_losses(train_losses)\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m---> 50\u001B[0m     main()\n",
      "Cell \u001B[0;32mIn[54], line 32\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     29\u001B[0m criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m---> 32\u001B[0m train_losses \u001B[38;5;241m=\u001B[39m train_model(model, train_loader, optimizer, criterion, epochs\u001B[38;5;241m=\u001B[39mnum_epochs, device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# Evaluate the model's performance\u001B[39;00m\n\u001B[1;32m     35\u001B[0m accuracy, precision, recall, f1 \u001B[38;5;241m=\u001B[39m evaluate_model(model, test_loader, device\u001B[38;5;241m=\u001B[39mdevice)\n",
      "Cell \u001B[0;32mIn[53], line 14\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, train_loader, optimizer, criterion, epochs, device)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(X\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m     13\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()  \u001B[38;5;66;03m# Clear gradients for the next train step\u001B[39;00m\n\u001B[0;32m---> 14\u001B[0m output \u001B[38;5;241m=\u001B[39m model(X)  \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[1;32m     15\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(output, labels)  \u001B[38;5;66;03m# Compute the loss\u001B[39;00m\n\u001B[1;32m     16\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()  \u001B[38;5;66;03m# Backward pass to compute gradients\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[51], line 24\u001B[0m, in \u001B[0;36mIMOTION_CNN.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     22\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mview(x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# Flattening the tensor\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(x\u001B[38;5;241m.\u001B[39mshape))\n\u001B[0;32m---> 24\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc1(x))\n\u001B[1;32m     25\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc2(x)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mlinear(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (50x256 and 576x200)"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T21:09:24.159151Z",
     "start_time": "2024-06-10T21:09:24.159089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assume input_tensor is the output of your convolutional layers with shape [50, 64, 9]\n",
    "input_tensor = torch.randn(50, 64, 9)\n",
    "\n",
    "# Flatten the last two dimensions of the tensor\n",
    "flattened = input_tensor.view(50, -1)  # Reshapes to [50, 576]"
   ],
   "id": "e5e4f5a6f06a53a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5fc224f9f492cd8d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
