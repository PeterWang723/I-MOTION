{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# I-MOTION Model ",
   "id": "d55c8b69028c2f30"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:55:14.165529Z",
     "start_time": "2024-06-11T15:55:13.406928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "window_size = 600\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")"
   ],
   "id": "3a1093f4cb8ad685",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available. Using CPU.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Data",
   "id": "22bcf9d0d02583f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:55:43.173869Z",
     "start_time": "2024-06-11T15:55:18.380356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data_x = np.loadtxt(\"./train/raw_data/Acc_x.txt\", delimiter=\" \")\n",
    "train_data_y = np.loadtxt(\"./train/raw_data/Acc_y.txt\", delimiter=\" \")\n",
    "train_data_z = np.loadtxt(\"./train/raw_data/Acc_z.txt\", delimiter=\" \")\n",
    "train_label = np.loadtxt(\"./train/train_label.txt\", delimiter=\" \")\n",
    "train_order = np.loadtxt(\"./train/train_order.txt\", dtype=float)\n",
    "acc_data = np.dstack((train_data_x, train_data_y, train_data_z, train_label))\n",
    "acc_data = acc_data[train_order.astype(np.int_).flatten() - 1, :, :]\n",
    "window_data = np.empty((acc_data.shape[0] * (acc_data.shape[1]//window_size), window_size, 3), dtype=np.float32)\n",
    "windowed_labels = np.empty((acc_data.shape[0] * (acc_data.shape[1]//window_size), 1), dtype=np.float32)\n",
    "\n",
    "window_index = 0\n",
    "for frame_index in range(acc_data.shape[0]):\n",
    "    \n",
    "    frame_features = acc_data[frame_index, :, :3]\n",
    "    frame_labels = acc_data[frame_index, :, 3]\n",
    "    \n",
    "    for start in range(0, acc_data.shape[1], window_size):\n",
    "        end = start + window_size\n",
    "        window_data[window_index] = frame_features[start:end, :]\n",
    "        # Assuming all samples in a window have the same label, take the label of the first sample\n",
    "        windowed_labels[window_index] = frame_labels[start]\n",
    "        window_index += 1\n",
    "        \n",
    "test_data_x = np.loadtxt(\"./test/test_raw_data/Acc_x.txt\", delimiter=\" \")\n",
    "test_data_y = np.loadtxt(\"./test/test_raw_data/Acc_y.txt\", delimiter=\" \")\n",
    "test_data_z = np.loadtxt(\"./test/test_raw_data/Acc_z.txt\", delimiter=\" \")\n",
    "test_label = np.loadtxt(\"./test/test_label.txt\", delimiter=\" \")\n",
    "test_order = np.loadtxt(\"./test/test_order.txt\", dtype=float)\n",
    "acc_data = np.dstack((test_data_x, test_data_y, test_data_z, test_label))\n",
    "acc_data = acc_data[test_order.astype(np.int_).flatten() - 1, :, :]\n",
    "test_window_data = np.empty((acc_data.shape[0] * (acc_data.shape[1]//window_size), window_size, 3), dtype=np.float32)\n",
    "test_windowed_labels = np.empty((acc_data.shape[0] * (acc_data.shape[1]//window_size), 1), dtype=np.float32)\n",
    "\n",
    "window_index = 0\n",
    "for frame_index in range(acc_data.shape[0]):\n",
    "    \n",
    "    frame_features = acc_data[frame_index, :, :3]\n",
    "    frame_labels = acc_data[frame_index, :, 3]\n",
    "    \n",
    "    for start in range(0, acc_data.shape[1], window_size):\n",
    "        end = start + window_size\n",
    "        test_window_data[window_index] = frame_features[start:end, :]\n",
    "        # Assuming all samples in a window have the same label, take the label of the first sample\n",
    "        test_windowed_labels[window_index] = frame_labels[start]\n",
    "        window_index += 1\n",
    "\n",
    "# Verify the shapes\n",
    "print(\"Train Features shape:\", window_data.shape)\n",
    "print(\"Train Labels shape:\", windowed_labels.shape)\n",
    "print(\"Test Features shape:\", test_window_data.shape)\n",
    "print(\"Test Labels shape:\", test_windowed_labels.shape)\n",
    "    "
   ],
   "id": "69106f9c06a7d448",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Features shape: (163100, 600, 3)\n",
      "Train Labels shape: (163100, 1)\n",
      "Test Features shape: (56980, 600, 3)\n",
      "Test Labels shape: (56980, 1)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Helper Function",
   "id": "98082d55a584d3f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:55:46.450232Z",
     "start_time": "2024-06-11T15:55:46.445124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_training_losses(train_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses)\n",
    "    plt.title('Training Loss Over Epochs', fontsize=16)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calculate_padding(kernel_size):\n",
    "    return (kernel_size - 1) // 2\n",
    "\n",
    "def calculate_output_length(input_length, kernel_size, stride, padding):\n",
    "    return ((input_length + 2 * padding - kernel_size) // stride) + 1\n",
    "\n",
    "def calculate_pooling_padding(window_size, stride, input_size, output_size):\n",
    "    return ((output_size - 1) * stride - input_size + window_size) // 2"
   ],
   "id": "ab1afb017abc5998",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "104355b2c124d563"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:56:13.669505Z",
     "start_time": "2024-06-11T15:55:47.267751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from scipy.signal import butter, filtfilt\n",
    "def preprocess_accelerometer_data(accel_data, m=5, cutoff=0.001, fs=100, order=5):\n",
    "    # Initialize gravity and linear acceleration\n",
    "    \n",
    "    # Step 1: Remove Gravity\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    linear_acceleration = filtfilt(b, a, accel_data, axis=0)\n",
    "    \n",
    "    # Step 2: Smooth Data\n",
    "    # Create a moving average (MA) filter\n",
    "    window = np.ones(m) / m\n",
    "    # Apply the moving average filter to each column\n",
    "    smoothed_data = np.zeros_like(linear_acceleration)\n",
    "    for i in range(linear_acceleration.shape[1]):\n",
    "        smoothed_data[:, i] = np.convolve(linear_acceleration[:, i], window, mode='same')\n",
    "\n",
    "    # Step 3: Calculate Magnitude\n",
    "    magnitudes = np.sqrt(np.sum(smoothed_data**2, axis=1))\n",
    "    return magnitudes\n",
    "\n",
    "\n",
    "# window_data size is 163100 x 600 x 3.\n",
    "train_processed_data = np.array([preprocess_accelerometer_data(window_data[i, :, :]) for i in range(window_data.shape[0])])\n",
    "test_processed_data = np.array([preprocess_accelerometer_data(test_window_data[i, :, :]) for i in range(test_window_data.shape[0])])\n",
    "print(train_processed_data.shape)  # Output the shape of the processed data\n",
    "print(test_processed_data.shape)"
   ],
   "id": "6c182e7b1d7a5ba5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(163100, 600)\n",
      "(56980, 600)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:56:17.895731Z",
     "start_time": "2024-06-11T15:56:17.890147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "for k in range(1, K + 1):  # k is 1-indexed in the mathematical formula\n",
    "        if k <= m // 2:\n",
    "            # Early data points: smaller window size that grows\n",
    "            smoothed_data[k - 1, :] = np.sum(linear_acceleration[:2 * k - 1, :], axis=0) / (2 * k - 1)\n",
    "        elif k > K - m // 2:\n",
    "            # Late data points: smaller window size that shrinks\n",
    "            smoothed_data[k - 1, :] = np.sum(linear_acceleration[2 * k - K - 1:, :]) / (2 * (K - k) + 1)\n",
    "        else:\n",
    "            # Middle data points: fixed window size\n",
    "            smoothed_data[k - 1, :] = np.sum(linear_acceleration[k - m // 2 - 1 : k + m // 2, :]) / m\n",
    "\"\"\""
   ],
   "id": "cf0f406ce8c9eae5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor k in range(1, K + 1):  # k is 1-indexed in the mathematical formula\\n        if k <= m // 2:\\n            # Early data points: smaller window size that grows\\n            smoothed_data[k - 1, :] = np.sum(linear_acceleration[:2 * k - 1, :], axis=0) / (2 * k - 1)\\n        elif k > K - m // 2:\\n            # Late data points: smaller window size that shrinks\\n            smoothed_data[k - 1, :] = np.sum(linear_acceleration[2 * k - K - 1:, :]) / (2 * (K - k) + 1)\\n        else:\\n            # Middle data points: fixed window size\\n            smoothed_data[k - 1, :] = np.sum(linear_acceleration[k - m // 2 - 1 : k + m // 2, :]) / m\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Construction",
   "id": "490b76b9e6eefb4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:56:18.706039Z",
     "start_time": "2024-06-11T15:56:18.702014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IMOTION_CNN(nn.Module):\n",
    "    def __init__(self, in_feature=600, in_channel=1, out_feature=8, pool_window_size=4, pool_stride_size=2, \n",
    "                 pool_padding=1, conv_filter_list=[32, 64, 64, 64, 64, 64], conv_kernel_list=[15, 10, 10, 5, 5, 5], conv_stride=1, full_connection_size=200):\n",
    "        super(IMOTION_CNN, self).__init__()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=pool_window_size, stride=pool_stride_size, padding=pool_padding)\n",
    "        assert len(conv_filter_list) == len(conv_kernel_list)\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=in_channel if i == 0 else conv_filter_list[i-1], \n",
    "                      out_channels=conv_filter_list[i], \n",
    "                      kernel_size=conv_kernel_list[i], stride=conv_stride, padding=calculate_padding(conv_kernel_list[i]))\n",
    "            for i in range(len(conv_filter_list))\n",
    "        ])\n",
    "        self.final_matrix_size = conv_filter_list[-1] * (in_feature // (2 ** (len(conv_filter_list))))\n",
    "        self.fc1 = nn.Linear(self.final_matrix_size, full_connection_size) \n",
    "        self.fc2 = nn.Linear(full_connection_size, out_feature)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv in self.conv_layers:\n",
    "            x = self.pool(torch.relu(conv(x)))\n",
    "        x = x.view(x.shape[0], -1)  # Flattening the tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "id": "c473c5560a7c999d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset",
   "id": "e45a94f47793432e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:56:19.863934Z",
     "start_time": "2024-06-11T15:56:19.859012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a custom Dataset\n",
    "class AccDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)  # Convert features to PyTorch tensors\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long) - 1       # Convert labels to PyTorch tensors\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "def prepare_data_loaders(train_features, train_labels, test_features, test_labels, batch_size=10):\n",
    "\n",
    "    train_dataset = AccDataset(train_features, train_labels)\n",
    "    test_dataset = AccDataset(test_features, test_labels)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ],
   "id": "7d2807cea662af84",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training & Evaluation",
   "id": "33cd9c130671e96c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:57:38.891068Z",
     "start_time": "2024-06-11T15:57:38.885529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, epochs=100, device='cpu'):\n",
    "    model.train()  # Set the model to training mode\n",
    "    loss_values = []  # Initialize a list to store the average loss per epoch\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0  # Track total loss for each epoch\n",
    "        for X, labels in train_loader:\n",
    "         \n",
    "            # Move data to the specified device\n",
    "            X, labels = X.to(device), labels.to(device)\n",
    "            X = X.unsqueeze(1)\n",
    "            optimizer.zero_grad()  # Clear gradients for the next train step\n",
    "            output = model(X)  # Forward pass\n",
    "            labels = labels.squeeze(1) \n",
    "            loss = criterion(output, labels)  # Compute the loss\n",
    "            loss.backward()  # Backward pass to compute gradients\n",
    "            optimizer.step()  # Update model parameters\n",
    "            total_loss += loss.item()  # Accumulate the loss\n",
    "         \n",
    "        avg_loss = total_loss / len(train_loader)  # Calculate average loss\n",
    "        loss_values.append(avg_loss)  # Append average loss to list\n",
    "\n",
    "        # Print the average loss for the current epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "    return loss_values\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, device='cpu'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    true_labels = []  # List to store actual labels\n",
    "    predictions = []  # List to store model predictions\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for X, labels in test_loader:\n",
    "            # Move data to the specified device\n",
    "            X, labels = X.to(device), labels.to(device)\n",
    "\n",
    "            output = model(X)  # Forward pass\n",
    "            _, predicted = torch.max(output.data, 1)  # Get the index of the max log-probability\n",
    "\n",
    "            true_labels += labels.tolist()  # Append actual labels\n",
    "            predictions += predicted.tolist()  # Append predicted labels\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='weighted')\n",
    "    recall = recall_score(true_labels, predictions, average='weighted')\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ],
   "id": "5d6a8d41cfa1c776",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main Script",
   "id": "855dcd082ae8e1f4"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-11T15:57:39.541767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function to train and evaluate Graph Convolutional Network (GCN) models\n",
    "    with different graph normalization techniques, visualize training metrics, and perform\n",
    "    embedding analysis through PCA.\n",
    "\n",
    "    Assumes the presence of a GCN model class, data loader preparation functions, and\n",
    "    various normalization technique functions defined outside this script.\n",
    "    \"\"\"\n",
    "    # Configuration parameters\n",
    "    num_epochs = 200  # Number of training epochs\n",
    "    # Dictionary mapping normalization technique names to their corresponding functions\n",
    "\n",
    "    # Lists for storing evaluation metrics and model information\n",
    "    metric_values = [[] for _ in range(4)]  # Lists to store Accuracy, Precision, Recall, F1 Score\n",
    "    train_losses = []  # Training loss values for each normalization technique\n",
    "\n",
    "    # Set the computation device (GPU or CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(f\"\\nTraining model\")\n",
    "    # Prepare data loaders\n",
    "    train_loader, test_loader = prepare_data_loaders(train_processed_data, windowed_labels, test_processed_data, test_windowed_labels, batch_size=100)\n",
    "    print(f\"DataLoader batch size: {train_loader.batch_size}\")\n",
    "\n",
    "    # Initialize the GCN model, optimizer, and loss criterion\n",
    "    model = IMOTION_CNN().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train the model\n",
    "    train_losses = train_model(model, train_loader, optimizer, criterion, epochs=num_epochs, device=device)\n",
    "\n",
    "    # Evaluate the model's performance\n",
    "    accuracy, precision, recall, f1 = evaluate_model(model, test_loader, device=device)\n",
    "    # Store the evaluation metrics\n",
    "    metric_values[0].append(accuracy)\n",
    "    metric_values[1].append(precision)\n",
    "    metric_values[2].append(recall)\n",
    "    metric_values[3].append(f1)\n",
    "\n",
    "    # Output the evaluation results\n",
    "    print(f\"Results  - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Visualization of training losses and evaluation metrics for each normalization technique\n",
    "    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    plot_training_losses(train_losses)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "78d42560882cae31",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model\n",
      "DataLoader batch size: 100\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T21:09:24.159151Z",
     "start_time": "2024-06-10T21:09:24.159089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assume input_tensor is the output of your convolutional layers with shape [50, 64, 9]\n",
    "input_tensor = torch.randn(50, 64, 9)\n",
    "\n",
    "# Flatten the last two dimensions of the tensor\n",
    "flattened = input_tensor.view(50, -1)  # Reshapes to [50, 576]"
   ],
   "id": "e5e4f5a6f06a53a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5fc224f9f492cd8d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
