{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# I-MOTION Model ",
   "id": "d55c8b69028c2f30"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T19:59:29.696190Z",
     "start_time": "2024-06-28T19:59:29.011673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "window_size = 600\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")"
   ],
   "id": "3a1093f4cb8ad685",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available. Using CPU.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Data",
   "id": "22bcf9d0d02583f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T19:59:58.577559Z",
     "start_time": "2024-06-28T19:59:33.243006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data_x = np.loadtxt(\"./data/Acc_x.txt\", delimiter=\" \")\n",
    "train_data_y = np.loadtxt(\"./data/Acc_y.txt\", delimiter=\" \")\n",
    "train_data_z = np.loadtxt(\"./data/Acc_z.txt\", delimiter=\" \")\n",
    "train_label = np.loadtxt(\"./data/train_label.txt\", delimiter=\" \")\n",
    "train_order = np.loadtxt(\"./data/train_order.txt\", dtype=float)\n",
    "acc_data = np.dstack((train_data_x, train_data_y, train_data_z, train_label))\n",
    "acc_data = acc_data[train_order.astype(np.int_).flatten() - 1, :, :]\n",
    "window_data = np.empty((acc_data.shape[0] * (acc_data.shape[1]//window_size), window_size, 3), dtype=np.float32)\n",
    "windowed_labels = np.empty((acc_data.shape[0] * (acc_data.shape[1]//window_size), 1), dtype=np.float32)\n",
    "\n",
    "window_index = 0\n",
    "for frame_index in range(acc_data.shape[0]):\n",
    "    \n",
    "    frame_features = acc_data[frame_index, :, :3]\n",
    "    frame_labels = acc_data[frame_index, :, 3]\n",
    "    \n",
    "    for start in range(0, acc_data.shape[1], window_size):\n",
    "        end = start + window_size\n",
    "        window_data[window_index] = frame_features[start:end, :]\n",
    "        # Assuming all samples in a window have the same label, take the label of the first sample\n",
    "        windowed_labels[window_index] = frame_labels[start]\n",
    "        window_index += 1\n",
    "        \n",
    "test_data_x = np.loadtxt(\"./data//test_Acc_x.txt\", delimiter=\" \")\n",
    "test_data_y = np.loadtxt(\"./data/test_Acc_y.txt\", delimiter=\" \")\n",
    "test_data_z = np.loadtxt(\"./data/test_Acc_z.txt\", delimiter=\" \")\n",
    "test_label = np.loadtxt(\"./data/test_label.txt\", delimiter=\" \")\n",
    "test_order = np.loadtxt(\"./data/test_order.txt\", dtype=float)\n",
    "acc_data = np.dstack((test_data_x, test_data_y, test_data_z, test_label))\n",
    "acc_data = acc_data[test_order.astype(np.int_).flatten() - 1, :, :]\n",
    "test_window_data = np.empty((acc_data.shape[0] * (acc_data.shape[1]//window_size), window_size, 3), dtype=np.float32)\n",
    "test_windowed_labels = np.empty((acc_data.shape[0] * (acc_data.shape[1]//window_size), 1), dtype=np.float32)\n",
    "\n",
    "window_index = 0\n",
    "for frame_index in range(acc_data.shape[0]):\n",
    "    \n",
    "    frame_features = acc_data[frame_index, :, :3]\n",
    "    frame_labels = acc_data[frame_index, :, 3]\n",
    "    \n",
    "    for start in range(0, acc_data.shape[1], window_size):\n",
    "        end = start + window_size\n",
    "        test_window_data[window_index] = frame_features[start:end, :]\n",
    "        # Assuming all samples in a window have the same label, take the label of the first sample\n",
    "        test_windowed_labels[window_index] = frame_labels[start]\n",
    "        window_index += 1\n",
    "\n",
    "# Verify the shapes\n",
    "print(\"Train Features shape:\", window_data.shape)\n",
    "print(\"Train Labels shape:\", windowed_labels.shape)\n",
    "print(\"Test Features shape:\", test_window_data.shape)\n",
    "print(\"Test Labels shape:\", test_windowed_labels.shape)\n",
    "    "
   ],
   "id": "69106f9c06a7d448",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Features shape: (163100, 600, 3)\n",
      "Train Labels shape: (163100, 1)\n",
      "Test Features shape: (56980, 600, 3)\n",
      "Test Labels shape: (56980, 1)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T19:11:24.360047Z",
     "start_time": "2024-06-27T19:09:01.852176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load train data\n",
    "train_data_x = np.loadtxt(\"./data/Acc_x.txt\", delimiter=\" \")\n",
    "train_data_y = np.loadtxt(\"./data/Acc_y.txt\", delimiter=\" \")\n",
    "train_data_z = np.loadtxt(\"./data/Acc_z.txt\", delimiter=\" \")\n",
    "train_label = np.loadtxt(\"./data/train_label.txt\", delimiter=\" \")\n",
    "train_order = np.loadtxt(\"./data/train_order.txt\", dtype=float)\n",
    "acc_data = np.dstack((train_data_x, train_data_y, train_data_z, train_label))\n",
    "acc_data = acc_data[train_order.astype(np.int_).flatten() - 1, :, :]\n",
    "\n",
    "# Calculate window data and labels for train data\n",
    "window_data = np.empty((acc_data.shape[0] * (acc_data.shape[1] // window_size), window_size, 3), dtype=np.float32)\n",
    "windowed_labels = np.empty((acc_data.shape[0] * (acc_data.shape[1] // window_size), 1), dtype=np.float32)\n",
    "\n",
    "window_index = 0\n",
    "for frame_index in range(acc_data.shape[0]):\n",
    "    frame_features = acc_data[frame_index, :, :3]\n",
    "    frame_labels = acc_data[frame_index, :, 3]\n",
    "    for start in range(0, acc_data.shape[1], window_size):\n",
    "        end = start + window_size\n",
    "        window_data[window_index] = frame_features[start:end, :]\n",
    "        windowed_labels[window_index] = frame_labels[start]\n",
    "        window_index += 1\n",
    "\n",
    "# Reshape and combine train data\n",
    "reshaped_train_data = window_data.reshape(-1, window_size * 3)\n",
    "combined_train_data = np.hstack((reshaped_train_data, windowed_labels))\n",
    "\n",
    "# Save train data to CSV\n",
    "pd.DataFrame(combined_train_data).to_csv('train_data.csv', index=False, header=True)\n",
    "\n",
    "# Verify the shapes\n",
    "print(\"Train Features shape:\", window_data.shape)\n",
    "print(\"Train Labels shape:\", windowed_labels.shape)\n",
    "print(combined_train_data.shape)"
   ],
   "id": "ebf5d0fd1f1dacaa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Features shape: (326200, 300, 3)\n",
      "Train Labels shape: (326200, 1)\n",
      "Test Features shape: (113960, 300, 3)\n",
      "Test Labels shape: (113960, 1)\n",
      "(326200, 901)\n",
      "[[ 2.047285  3.183076 10.796031 ...  3.2432   10.028386  5.      ]\n",
      " [ 1.844691  3.778356  9.555554 ...  2.31379   8.477245  5.      ]\n",
      " [ 1.475761  2.650088  9.071687 ...  3.246537  9.30294   5.      ]\n",
      " ...\n",
      " [-3.704407  0.064732 -9.034916 ...  0.237594 -9.693662  5.      ]\n",
      " [-3.394724 -0.0289   -9.846091 ... -0.029576 -9.040967  5.      ]\n",
      " [-3.621687 -0.073742 -8.714023 ... -0.017882 -9.637794  5.      ]]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T19:24:16.395716Z",
     "start_time": "2024-06-27T19:24:16.390582Z"
    }
   },
   "cell_type": "code",
   "source": "print(combined_train_data.shape)",
   "id": "841adb6a8976844f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(326200, 901)\n",
      "(113960, 901)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T11:36:21.021568Z",
     "start_time": "2024-06-28T11:34:35.636684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file without headers\n",
    "data = pd.read_csv('train_data.csv', header=None)\n",
    "\n",
    "# Generate column names for the accelerometer data and label\n",
    "column_names = [f'Acc_{i//3+1}_{[\"x\", \"y\", \"z\"][i%3]}' for i in range(900)]\n",
    "column_names.append('Label')\n",
    "\n",
    "# Set the column names to the DataFrame\n",
    "data.columns = column_names\n",
    "\n",
    "# Convert 'Label' column to integer type\n",
    "data['Label'] = data['Label'].astype(int)\n",
    "\n",
    "# Save the DataFrame back to a CSV file with column names\n",
    "data.to_csv('train_data_with_headers.csv', index=False)\n",
    "\n",
    "# Verify the first few rows and data types to ensure everything is set up correctly\n",
    "print(data.head())\n",
    "print(data.dtypes)"
   ],
   "id": "251fdf4053ca02f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Acc_1_x    Acc_1_y   Acc_1_z   Acc_2_x    Acc_2_y   Acc_2_z   Acc_3_x  \\\n",
      "0 -2.367480   9.603528 -1.200628 -2.442098   9.238089 -1.462477 -2.774196   \n",
      "1 -0.478840   9.461885  0.900801 -0.446564   9.511542  0.975061 -0.433618   \n",
      "2 -1.367722   9.750951  0.306458 -1.334744   9.776136  0.295097 -1.316464   \n",
      "3 -0.374039   7.868162  0.633156 -0.464943   8.151039  0.820221 -0.482534   \n",
      "4  2.160904  13.322191 -1.865706  2.389481  12.746821 -1.159406  2.072968   \n",
      "\n",
      "     Acc_3_y   Acc_3_z   Acc_4_x  ...  Acc_298_x  Acc_298_y  Acc_298_z  \\\n",
      "0   8.611417 -1.739763 -3.246977  ...  -0.702284   9.517571   0.879291   \n",
      "1   9.535838  0.963800 -0.402226  ...  -1.525542   9.455682   0.303084   \n",
      "2   9.764919  0.287304 -1.258937  ...  -0.058308   8.344917   0.338663   \n",
      "3   8.718449  0.901451 -0.685306  ...   0.137368  12.029835  -2.380630   \n",
      "4  12.296709 -1.122953  2.250746  ...  -2.110125   7.896227  -1.579852   \n",
      "\n",
      "   Acc_299_x  Acc_299_y  Acc_299_z  Acc_300_x  Acc_300_y  Acc_300_z  Label  \n",
      "0  -0.585128   9.503646   0.857516  -0.507571   9.502698   0.865448      7  \n",
      "1  -1.469803   9.531071   0.279402  -1.421543   9.670200   0.285504      7  \n",
      "2  -0.143652   8.168410   0.376890  -0.228872   8.002018   0.436869      7  \n",
      "3   0.710383  13.127102  -2.832197   1.297946  13.610430  -2.181224      7  \n",
      "4  -2.777274   7.186439  -2.099240  -3.020220   8.931874  -2.813815      7  \n",
      "\n",
      "[5 rows x 901 columns]\n",
      "Acc_1_x      float64\n",
      "Acc_1_y      float64\n",
      "Acc_1_z      float64\n",
      "Acc_2_x      float64\n",
      "Acc_2_y      float64\n",
      "              ...   \n",
      "Acc_299_z    float64\n",
      "Acc_300_x    float64\n",
      "Acc_300_y    float64\n",
      "Acc_300_z    float64\n",
      "Label          int64\n",
      "Length: 901, dtype: object\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T19:30:05.275469Z",
     "start_time": "2024-06-27T19:30:00.789708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "combined_data.columns = combined_data.iloc[0]\n",
    "combined_data = combined_data[1:].reset_index(drop=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(combined_data.head())"
   ],
   "id": "3e3dac2c08796ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   Acc_1_x    Acc_1_y   Acc_1_z   Acc_2_x    Acc_2_y   Acc_2_z   Acc_3_x  \\\n",
      "0  -2.36748   9.603528 -1.200628 -2.442098   9.238089 -1.462477 -2.774196   \n",
      "1  -0.47884   9.461885  0.900801 -0.446564   9.511542  0.975061 -0.433618   \n",
      "2 -1.367722   9.750951  0.306458 -1.334744   9.776136  0.295097 -1.316464   \n",
      "3 -0.374039   7.868162  0.633156 -0.464943   8.151039  0.820221 -0.482534   \n",
      "4  2.160904  13.322191 -1.865706  2.389481  12.746821 -1.159406  2.072968   \n",
      "\n",
      "0    Acc_3_y   Acc_3_z   Acc_4_x  ... Acc_298_x  Acc_298_y Acc_298_z  \\\n",
      "0   8.611417 -1.739763 -3.246977  ... -0.702284   9.517571  0.879291   \n",
      "1   9.535838    0.9638 -0.402226  ... -1.525542   9.455682  0.303084   \n",
      "2   9.764919  0.287304 -1.258937  ... -0.058308   8.344917  0.338663   \n",
      "3   8.718449  0.901451 -0.685306  ...  0.137368  12.029835  -2.38063   \n",
      "4  12.296709 -1.122953  2.250746  ... -2.110125   7.896227 -1.579852   \n",
      "\n",
      "0 Acc_299_x  Acc_299_y Acc_299_z Acc_300_x Acc_300_y Acc_300_z Label  \n",
      "0 -0.585128   9.503646  0.857516 -0.507571  9.502698  0.865448   7.0  \n",
      "1 -1.469803   9.531071  0.279402 -1.421543    9.6702  0.285504   7.0  \n",
      "2 -0.143652    8.16841   0.37689 -0.228872  8.002018  0.436869   7.0  \n",
      "3  0.710383  13.127102 -2.832197  1.297946  13.61043 -2.181224   7.0  \n",
      "4 -2.777274   7.186439  -2.09924  -3.02022  8.931874 -2.813815   7.0  \n",
      "\n",
      "[5 rows x 901 columns]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T19:42:10.474774Z",
     "start_time": "2024-06-27T19:40:36.315805Z"
    }
   },
   "cell_type": "code",
   "source": "combined_data.to_csv('train_data_with_headers_no.csv', index=False, header=False)",
   "id": "8129cf1aa8e4c5d7",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T19:35:21.610404Z",
     "start_time": "2024-06-27T19:35:21.607107Z"
    }
   },
   "cell_type": "code",
   "source": "print(combined_data.columns)",
   "id": "44a2c9708ab1e35d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Acc_1_x', 'Acc_1_y', 'Acc_1_z', 'Acc_2_x', 'Acc_2_y', 'Acc_2_z',\n",
      "       'Acc_3_x', 'Acc_3_y', 'Acc_3_z', 'Acc_4_x',\n",
      "       ...\n",
      "       'Acc_298_x', 'Acc_298_y', 'Acc_298_z', 'Acc_299_x', 'Acc_299_y',\n",
      "       'Acc_299_z', 'Acc_300_x', 'Acc_300_y', 'Acc_300_z', 'Label'],\n",
      "      dtype='object', name=0, length=901)\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Helper Function",
   "id": "98082d55a584d3f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:55:46.450232Z",
     "start_time": "2024-06-11T15:55:46.445124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_training_losses(train_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses)\n",
    "    plt.title('Training Loss Over Epochs', fontsize=16)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calculate_padding(kernel_size):\n",
    "    return (kernel_size - 1) // 2\n",
    "\n",
    "def calculate_output_length(input_length, kernel_size, stride, padding):\n",
    "    return ((input_length + 2 * padding - kernel_size) // stride) + 1\n",
    "\n",
    "def calculate_pooling_padding(window_size, stride, input_size, output_size):\n",
    "    return ((output_size - 1) * stride - input_size + window_size) // 2"
   ],
   "id": "ab1afb017abc5998",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "104355b2c124d563"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T20:00:25.331088Z",
     "start_time": "2024-06-28T19:59:58.579185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from scipy.signal import butter, filtfilt\n",
    "def preprocess_accelerometer_data(accel_data, m=5, cutoff=0.001, fs=100, order=5):\n",
    "    # Initialize gravity and linear acceleration\n",
    "    \n",
    "    # Step 1: Remove Gravity\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    linear_acceleration = filtfilt(b, a, accel_data, axis=0)\n",
    "    \n",
    "    # Step 2: Smooth Data\n",
    "    # Create a moving average (MA) filter\n",
    "    window = np.ones(m) / m\n",
    "    # Apply the moving average filter to each column\n",
    "    smoothed_data = np.zeros_like(linear_acceleration)\n",
    "    for i in range(linear_acceleration.shape[1]):\n",
    "        smoothed_data[:, i] = np.convolve(linear_acceleration[:, i], window, mode='same')\n",
    "\n",
    "    # Step 3: Calculate Magnitude\n",
    "    magnitudes = np.sqrt(np.sum(smoothed_data**2, axis=1))\n",
    "    return magnitudes\n",
    "\n",
    "\n",
    "# window_data size is 163100 x 600 x 3.\n",
    "train_processed_data = np.array([preprocess_accelerometer_data(window_data[i, :, :]) for i in range(window_data.shape[0])])\n",
    "test_processed_data = np.array([preprocess_accelerometer_data(test_window_data[i, :, :]) for i in range(test_window_data.shape[0])])\n",
    "print(train_processed_data.shape)  # Output the shape of the processed data\n",
    "print(test_processed_data.shape)"
   ],
   "id": "6c182e7b1d7a5ba5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(163100, 600)\n",
      "(56980, 600)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T20:01:14.405834Z",
     "start_time": "2024-06-28T20:01:14.401200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(train_processed_data.shape) \n",
    "print(windowed_labels.shape)"
   ],
   "id": "bc2566149759a25f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(163100, 600)\n",
      "(163100, 600, 3)\n",
      "(163100, 1)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T20:06:47.986122Z",
     "start_time": "2024-06-28T20:06:01.045568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "feature_columns = [f'acc_{i+1}' for i in range(train_processed_data.shape[1])]\n",
    "# Label column\n",
    "label_column = ['label']\n",
    "\n",
    "# All columns\n",
    "columns = feature_columns + label_column\n",
    "\n",
    "# Step 3: Combine Features and Labels\n",
    "# Combine features and labels into a single DataFrame\n",
    "combined_data = np.hstack((train_processed_data, windowed_labels))\n",
    "df = pd.DataFrame(combined_data, columns=columns)\n",
    "\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Step 4: Save to CSV File\n",
    "df.to_csv('combined_data.csv', index=False)\n",
    "print(df.head())\n",
    "print(df.dtypes)"
   ],
   "id": "9a997d7704b903b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      acc_1     acc_2     acc_3     acc_4     acc_5     acc_6     acc_7  \\\n",
      "0  3.735548  4.948487  6.411320  6.619566  6.842094  7.112462  7.359571   \n",
      "1  3.872650  5.152979  6.423147  6.386667  6.343330  6.319507  6.327529   \n",
      "2  4.474486  5.870930  7.314752  7.093672  6.896525  6.703823  6.217646   \n",
      "3  3.723725  5.046302  6.432362  6.671743  6.930723  7.220342  7.515652   \n",
      "4  3.919540  5.192972  6.446633  6.381846  6.298956  6.238860  6.211018   \n",
      "\n",
      "      acc_8     acc_9    acc_10  ...   acc_592   acc_593   acc_594   acc_595  \\\n",
      "0  7.400199  7.407766  7.372681  ...  6.480283  6.513780  6.513585  6.490097   \n",
      "1  6.344512  6.371496  6.384150  ...  5.520541  5.249839  4.982332  5.092405   \n",
      "2  5.528437  4.674045  3.693581  ...  6.199013  6.201717  6.144378  6.026480   \n",
      "3  7.797378  8.033084  8.155891  ...  6.404034  6.434877  6.483223  6.540555   \n",
      "4  6.215341  6.243834  6.298971  ...  6.745467  6.709211  6.671676  6.623878   \n",
      "\n",
      "    acc_596   acc_597   acc_598   acc_599   acc_600  label  \n",
      "0  6.447406  6.425032  6.433006  5.141509  3.861026      7  \n",
      "1  5.631380  6.559002  7.658864  6.729902  5.442239      7  \n",
      "2  5.882927  5.727175  5.608477  4.436586  3.290293      7  \n",
      "3  6.571757  6.576472  6.555757  5.229339  3.900198      7  \n",
      "4  6.559768  6.516585  6.475703  5.160044  3.855894      7  \n",
      "\n",
      "[5 rows x 601 columns]\n",
      "acc_1      float64\n",
      "acc_2      float64\n",
      "acc_3      float64\n",
      "acc_4      float64\n",
      "acc_5      float64\n",
      "            ...   \n",
      "acc_597    float64\n",
      "acc_598    float64\n",
      "acc_599    float64\n",
      "acc_600    float64\n",
      "label        int64\n",
      "Length: 601, dtype: object\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Construction",
   "id": "490b76b9e6eefb4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:56:18.706039Z",
     "start_time": "2024-06-11T15:56:18.702014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IMOTION_CNN(nn.Module):\n",
    "    def __init__(self, in_feature=600, in_channel=1, out_feature=8, pool_window_size=4, pool_stride_size=2, \n",
    "                 pool_padding=1, conv_filter_list=[32, 64, 64, 64, 64, 64], conv_kernel_list=[15, 10, 10, 5, 5, 5], conv_stride=1, full_connection_size=200):\n",
    "        super(IMOTION_CNN, self).__init__()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=pool_window_size, stride=pool_stride_size, padding=pool_padding)\n",
    "        assert len(conv_filter_list) == len(conv_kernel_list)\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=in_channel if i == 0 else conv_filter_list[i-1], \n",
    "                      out_channels=conv_filter_list[i], \n",
    "                      kernel_size=conv_kernel_list[i], stride=conv_stride, padding=calculate_padding(conv_kernel_list[i]))\n",
    "            for i in range(len(conv_filter_list))\n",
    "        ])\n",
    "        self.final_matrix_size = conv_filter_list[-1] * (in_feature // (2 ** (len(conv_filter_list))))\n",
    "        self.fc1 = nn.Linear(self.final_matrix_size, full_connection_size) \n",
    "        self.fc2 = nn.Linear(full_connection_size, out_feature)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv in self.conv_layers:\n",
    "            x = self.pool(conv(x))\n",
    "            \n",
    "        x = x.view(x.shape[0], -1)  # Flattening the tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "id": "c473c5560a7c999d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset",
   "id": "e45a94f47793432e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:56:19.863934Z",
     "start_time": "2024-06-11T15:56:19.859012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a custom Dataset\n",
    "class AccDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)  # Convert features to PyTorch tensors\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long) - 1       # Convert labels to PyTorch tensors\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "def prepare_data_loaders(train_features, train_labels, test_features, test_labels, batch_size=10):\n",
    "\n",
    "    train_dataset = AccDataset(train_features, train_labels)\n",
    "    test_dataset = AccDataset(test_features, test_labels)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ],
   "id": "7d2807cea662af84",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training & Evaluation",
   "id": "33cd9c130671e96c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:57:38.891068Z",
     "start_time": "2024-06-11T15:57:38.885529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, epochs=100, device='cpu'):\n",
    "    model.train()  # Set the model to training mode\n",
    "    loss_values = []  # Initialize a list to store the average loss per epoch\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0  # Track total loss for each epoch\n",
    "        for X, labels in train_loader:\n",
    "         \n",
    "            # Move data to the specified device\n",
    "            X, labels = X.to(device), labels.to(device)\n",
    "            X = X.unsqueeze(1)\n",
    "            optimizer.zero_grad()  # Clear gradients for the next train step\n",
    "            output = model(X)  # Forward pass\n",
    "            labels = labels.squeeze(1) \n",
    "            loss = criterion(output, labels)  # Compute the loss\n",
    "            loss.backward()  # Backward pass to compute gradients\n",
    "            optimizer.step()  # Update model parameters\n",
    "            total_loss += loss.item()  # Accumulate the loss\n",
    "         \n",
    "        avg_loss = total_loss / len(train_loader)  # Calculate average loss\n",
    "        loss_values.append(avg_loss)  # Append average loss to list\n",
    "\n",
    "        # Print the average loss for the current epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "    return loss_values\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, device='cpu'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    true_labels = []  # List to store actual labels\n",
    "    predictions = []  # List to store model predictions\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for X, labels in test_loader:\n",
    "            # Move data to the specified device\n",
    "            X, labels = X.to(device), labels.to(device)\n",
    "\n",
    "            output = model(X)  # Forward pass\n",
    "            _, predicted = torch.max(output.data, 1)  # Get the index of the max log-probability\n",
    "\n",
    "            true_labels += labels.tolist()  # Append actual labels\n",
    "            predictions += predicted.tolist()  # Append predicted labels\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='weighted')\n",
    "    recall = recall_score(true_labels, predictions, average='weighted')\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ],
   "id": "5d6a8d41cfa1c776",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main Script",
   "id": "855dcd082ae8e1f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T19:53:00.204094Z",
     "start_time": "2024-06-11T15:57:39.541767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function to train and evaluate Graph Convolutional Network (GCN) models\n",
    "    with different graph normalization techniques, visualize training metrics, and perform\n",
    "    embedding analysis through PCA.\n",
    "\n",
    "    Assumes the presence of a GCN model class, data loader preparation functions, and\n",
    "    various normalization technique functions defined outside this script.\n",
    "    \"\"\"\n",
    "    # Configuration parameters\n",
    "    num_epochs = 200  # Number of training epochs\n",
    "    # Dictionary mapping normalization technique names to their corresponding functions\n",
    "\n",
    "    # Lists for storing evaluation metrics and model information\n",
    "    metric_values = [[] for _ in range(4)]  # Lists to store Accuracy, Precision, Recall, F1 Score\n",
    "    train_losses = []  # Training loss values for each normalization technique\n",
    "\n",
    "    # Set the computation device (GPU or CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(f\"\\nTraining model\")\n",
    "    # Prepare data loaders\n",
    "    train_loader, test_loader = prepare_data_loaders(train_processed_data, windowed_labels, test_processed_data, test_windowed_labels, batch_size=100)\n",
    "    print(f\"DataLoader batch size: {train_loader.batch_size}\")\n",
    "\n",
    "    # Initialize the GCN model, optimizer, and loss criterion\n",
    "    model = IMOTION_CNN().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train the model\n",
    "    train_losses = train_model(model, train_loader, optimizer, criterion, epochs=num_epochs, device=device)\n",
    "\n",
    "    # Evaluate the model's performance\n",
    "    accuracy, precision, recall, f1 = evaluate_model(model, test_loader, device=device)\n",
    "    # Store the evaluation metrics\n",
    "    metric_values[0].append(accuracy)\n",
    "    metric_values[1].append(precision)\n",
    "    metric_values[2].append(recall)\n",
    "    metric_values[3].append(f1)\n",
    "\n",
    "    # Output the evaluation results\n",
    "    print(f\"Results  - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Visualization of training losses and evaluation metrics for each normalization technique\n",
    "    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    plot_training_losses(train_losses)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "78d42560882cae31",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model\n",
      "DataLoader batch size: 100\n",
      "Epoch 1/200, Loss: 1.0091\n",
      "Epoch 2/200, Loss: 0.8350\n",
      "Epoch 3/200, Loss: 0.7858\n",
      "Epoch 4/200, Loss: 0.7485\n",
      "Epoch 5/200, Loss: 0.7171\n",
      "Epoch 6/200, Loss: 0.7190\n",
      "Epoch 7/200, Loss: 0.6916\n",
      "Epoch 8/200, Loss: 0.6754\n",
      "Epoch 9/200, Loss: 0.6637\n",
      "Epoch 10/200, Loss: 0.6428\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 50\u001B[0m\n\u001B[1;32m     47\u001B[0m     plot_training_losses(train_losses)\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m---> 50\u001B[0m     main()\n",
      "Cell \u001B[0;32mIn[11], line 32\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     29\u001B[0m criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m---> 32\u001B[0m train_losses \u001B[38;5;241m=\u001B[39m train_model(model, train_loader, optimizer, criterion, epochs\u001B[38;5;241m=\u001B[39mnum_epochs, device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# Evaluate the model's performance\u001B[39;00m\n\u001B[1;32m     35\u001B[0m accuracy, precision, recall, f1 \u001B[38;5;241m=\u001B[39m evaluate_model(model, test_loader, device\u001B[38;5;241m=\u001B[39mdevice)\n",
      "Cell \u001B[0;32mIn[10], line 13\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, train_loader, optimizer, criterion, epochs, device)\u001B[0m\n\u001B[1;32m     11\u001B[0m X \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     12\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()  \u001B[38;5;66;03m# Clear gradients for the next train step\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m output \u001B[38;5;241m=\u001B[39m model(X)  \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[1;32m     14\u001B[0m labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m1\u001B[39m) \n\u001B[1;32m     15\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(output, labels)  \u001B[38;5;66;03m# Compute the loss\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[6], line 20\u001B[0m, in \u001B[0;36mIMOTION_CNN.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m conv \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv_layers:\n\u001B[0;32m---> 20\u001B[0m         x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool(torch\u001B[38;5;241m.\u001B[39mrelu(conv(x)))\n\u001B[1;32m     21\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mview(x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# Flattening the tensor\u001B[39;00m\n\u001B[1;32m     22\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc1(x))\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:310\u001B[0m, in \u001B[0;36mConv1d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conv_forward(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:306\u001B[0m, in \u001B[0;36mConv1d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    302\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    303\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv1d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    304\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    305\u001B[0m                     _single(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 306\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv1d(\u001B[38;5;28minput\u001B[39m, weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    307\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T21:09:24.159151Z",
     "start_time": "2024-06-10T21:09:24.159089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assume input_tensor is the output of your convolutional layers with shape [50, 64, 9]\n",
    "input_tensor = torch.randn(50, 64, 9)\n",
    "\n",
    "# Flatten the last two dimensions of the tensor\n",
    "flattened = input_tensor.view(50, -1)  # Reshapes to [50, 576]"
   ],
   "id": "e5e4f5a6f06a53a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5fc224f9f492cd8d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
