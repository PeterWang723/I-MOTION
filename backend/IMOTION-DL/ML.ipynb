{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# I-MOTION Model ",
   "id": "d55c8b69028c2f30"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T12:38:43.786830Z",
     "start_time": "2024-05-24T12:38:43.063299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "window_size = 600\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")"
   ],
   "id": "3a1093f4cb8ad685",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available. Using CPU.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Data",
   "id": "22bcf9d0d02583f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T12:39:08.114222Z",
     "start_time": "2024-05-24T12:38:51.160338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data_x = np.loadtxt(\"./train/raw_data/Acc_x.txt\", delimiter=\" \")\n",
    "train_data_y = np.loadtxt(\"./train/raw_data/Acc_y.txt\", delimiter=\" \")\n",
    "train_data_z = np.loadtxt(\"./train/raw_data/Acc_z.txt\", delimiter=\" \")\n",
    "train_label = np.loadtxt(\"./train/train_label.txt\", delimiter=\" \")\n",
    "train_order = np.loadtxt(\"./train/train_order.txt\", dtype=float)\n",
    "acc_data = np.dstack((train_data_x, train_data_y, train_data_z, train_label))\n",
    "acc_data = acc_data[train_order.astype(np.int_).flatten() - 1, :, :]\n",
    "window_data = np.empty((acc_data.shape[0] * (acc_data.shape[1]//window_size), window_size, 3), dtype=np.float32)\n",
    "windowed_labels = np.empty((acc_data.shape[0] * (acc_data.shape[1]//window_size), 1), dtype=np.float32)\n",
    "\n",
    "window_index = 0\n",
    "for frame_index in range(acc_data.shape[0]):\n",
    "    \n",
    "    frame_features = acc_data[frame_index, :, :3]\n",
    "    frame_labels = acc_data[frame_index, :, 3]\n",
    "    \n",
    "    for start in range(0, acc_data.shape[1], window_size):\n",
    "        end = start + window_size\n",
    "        window_data[window_index] = frame_features[start:end, :]\n",
    "        # Assuming all samples in a window have the same label, take the label of the first sample\n",
    "        windowed_labels[window_index] = frame_labels[start]\n",
    "        window_index += 1\n",
    "        \n",
    "test_data_x = np.loadtxt(\"./test/test_raw_data/Acc_x.txt\", delimiter=\" \")\n",
    "test_data_y = np.loadtxt(\"./test/test_raw_data/Acc_y.txt\", delimiter=\" \")\n",
    "test_data_z = np.loadtxt(\"./test/test_raw_data/Acc_z.txt\", delimiter=\" \")\n",
    "test_label = np.loadtxt(\"./test/test_label.txt\", delimiter=\" \")\n",
    "test_order = np.loadtxt(\"./test/test_order.txt\", dtype=float)\n",
    "acc_data = np.dstack((test_data_x, test_data_y, test_data_z, test_label))\n",
    "acc_data = acc_data[train_order.astype(np.int_).flatten() - 1, :, :]\n",
    "test_window_data = np.empty((acc_data.shape[0] * (acc_data.shape[1]//window_size), window_size, 3), dtype=np.float32)\n",
    "test_windowed_labels = np.empty((acc_data.shape[0] * (acc_data.shape[1]//window_size), 1), dtype=np.float32)\n",
    "\n",
    "window_index = 0\n",
    "for frame_index in range(acc_data.shape[0]):\n",
    "    \n",
    "    frame_features = acc_data[frame_index, :, :3]\n",
    "    frame_labels = acc_data[frame_index, :, 3]\n",
    "    \n",
    "    for start in range(0, acc_data.shape[1], window_size):\n",
    "        end = start + window_size\n",
    "        test_window_data[window_index] = frame_features[start:end, :]\n",
    "        # Assuming all samples in a window have the same label, take the label of the first sample\n",
    "        test_windowed_labels[window_index] = frame_labels[start]\n",
    "        window_index += 1\n",
    "\n",
    "# Verify the shapes\n",
    "print(\"Train Features shape:\", window_data.shape)\n",
    "print(\"Train Labels shape:\", windowed_labels.shape)\n",
    "print(\"Test Features shape:\", test_window_data.shape)\n",
    "print(\"Test Labels shape:\", test_windowed_labels.shape)\n",
    "    "
   ],
   "id": "69106f9c06a7d448",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (163100, 600, 3)\n",
      "Labels shape: (163100, 1)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Helper Function",
   "id": "98082d55a584d3f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T13:50:57.696271Z",
     "start_time": "2024-05-24T13:50:57.675090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_training_losses(train_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses)\n",
    "    plt.title('Training Loss Over Epochs', fontsize=16)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calculate_padding(kernel_size):\n",
    "    return (kernel_size - 1) // 2\n",
    "\n",
    "def calculate_output_length(input_length, kernel_size, stride, padding):\n",
    "    return ((input_length + 2 * padding - kernel_size) // stride) + 1\n",
    "\n",
    "def calculate_pooling_padding(window_size, stride, input_size, output_size):\n",
    "    return ((output_size - 1) * stride - input_size + window_size) // 2"
   ],
   "id": "ab1afb017abc5998",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "104355b2c124d563"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T16:11:09.636633Z",
     "start_time": "2024-05-24T16:10:52.215227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from scipy.signal import butter, filtfilt\n",
    "def preprocess_accelerometer_data(accel_data, m=5, cutoff=0.001, fs=100, order=5):\n",
    "    # Initialize gravity and linear acceleration\n",
    "    \n",
    "    # Step 1: Remove Gravity\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    linear_acceleration = filtfilt(b, a, accel_data, axis=0)\n",
    "    \n",
    "    # Step 2: Smooth Data\n",
    "    # Create a moving average (MA) filter\n",
    "    window = np.ones(m) / m\n",
    "    # Apply the moving average filter to each column\n",
    "    smoothed_data = np.zeros_like(linear_acceleration)\n",
    "    for i in range(linear_acceleration.shape[1]):\n",
    "        smoothed_data[:, i] = np.convolve(linear_acceleration[:, i], window, mode='same')\n",
    "\n",
    "    # Step 3: Calculate Magnitude\n",
    "    magnitudes = np.sqrt(np.sum(smoothed_data**2, axis=1))\n",
    "    return magnitudes\n",
    "\n",
    "\n",
    "# window_data size is 163100 x 600 x 3.\n",
    "processed_data = np.array([preprocess_accelerometer_data(window_data[i, :, :]) for i in range(window_data.shape[0])])\n",
    "\n",
    "print(processed_data.shape)  # Output the shape of the processed data\n"
   ],
   "id": "6c182e7b1d7a5ba5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(163100, 600)\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "for k in range(1, K + 1):  # k is 1-indexed in the mathematical formula\n",
    "        if k <= m // 2:\n",
    "            # Early data points: smaller window size that grows\n",
    "            smoothed_data[k - 1, :] = np.sum(linear_acceleration[:2 * k - 1, :], axis=0) / (2 * k - 1)\n",
    "        elif k > K - m // 2:\n",
    "            # Late data points: smaller window size that shrinks\n",
    "            smoothed_data[k - 1, :] = np.sum(linear_acceleration[2 * k - K - 1:, :]) / (2 * (K - k) + 1)\n",
    "        else:\n",
    "            # Middle data points: fixed window size\n",
    "            smoothed_data[k - 1, :] = np.sum(linear_acceleration[k - m // 2 - 1 : k + m // 2, :]) / m\n",
    "\"\"\""
   ],
   "id": "cf0f406ce8c9eae5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Construction",
   "id": "490b76b9e6eefb4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T16:11:25.121997Z",
     "start_time": "2024-05-24T16:11:25.116858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IMOTION_CNN(nn.Module):\n",
    "    def __init__(self, in_feature=0, in_channel=0, out_feature=0, pool_window_size=0, pool_stride_size=0, \n",
    "                 pool_padding=0, conv_filter_list=0, conv_kernel_list=0, conv_stride=0, full_connection_size=0):\n",
    "        super(IMOTION_CNN, self).__init__()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=pool_window_size, stride=pool_stride_size, padding=pool_padding)\n",
    "        assert len(conv_filter_list) == len(conv_kernel_list)\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=in_channel if i == 0 else conv_filter_list[i-1], \n",
    "                      out_channels=conv_filter_list[i], \n",
    "                      kernel_size=conv_kernel_list[i], stride=conv_stride)\n",
    "            for i in range(len(conv_filter_list))\n",
    "        ])\n",
    "        self.final_matrix_size = conv_filter_list[-1] * (in_feature / (2 ** len(conv_filter_list)))\n",
    "        self.fc1 = nn.Linear(self.final_matrix_size, full_connection_size) \n",
    "        self.fc2 = nn.Linear(full_connection_size, out_feature)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv in self.conv_layers:\n",
    "            x = self.pool(torch.relu(conv(x)))\n",
    "        x = x.view(-1, self.final_matrix_size)  # Flattening the tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "id": "c473c5560a7c999d",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset",
   "id": "e45a94f47793432e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T16:11:42.103757Z",
     "start_time": "2024-05-24T16:11:42.097933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a custom Dataset\n",
    "class AccDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)  # Convert features to PyTorch tensors\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)       # Convert labels to PyTorch tensors\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "def prepare_data_loaders(train_features, train_labels, test_features, test_labels, batch_size=10):\n",
    "\n",
    "    train_dataset = AccDataset(train_features, train_labels)\n",
    "    test_dataset = AccDataset(test_features, test_labels)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ],
   "id": "7d2807cea662af84",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training & Evaluation",
   "id": "33cd9c130671e96c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T16:11:45.870213Z",
     "start_time": "2024-05-24T16:11:45.867022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, epochs=100, device='cpu'):\n",
    "    model.train()  # Set the model to training mode\n",
    "    loss_values = []  # Initialize a list to store the average loss per epoch\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0  # Track total loss for each epoch\n",
    "\n",
    "        for X, labels in train_loader:\n",
    "            # Move data to the specified device\n",
    "            X, labels = X.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Clear gradients for the next train step\n",
    "            output = model(X)  # Forward pass\n",
    "            loss = criterion(output, labels)  # Compute the loss\n",
    "            loss.backward()  # Backward pass to compute gradients\n",
    "            optimizer.step()  # Update model parameters\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate the loss\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)  # Calculate average loss\n",
    "        loss_values.append(avg_loss)  # Append average loss to list\n",
    "\n",
    "        # Print the average loss for the current epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "    return loss_values\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, device='cpu'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    true_labels = []  # List to store actual labels\n",
    "    predictions = []  # List to store model predictions\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for X, labels in test_loader:\n",
    "            # Move data to the specified device\n",
    "            X, labels = X.to(device), labels.to(device)\n",
    "\n",
    "            output = model(X)  # Forward pass\n",
    "            _, predicted = torch.max(output.data, 1)  # Get the index of the max log-probability\n",
    "\n",
    "            true_labels += labels.tolist()  # Append actual labels\n",
    "            predictions += predicted.tolist()  # Append predicted labels\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='weighted')\n",
    "    recall = recall_score(true_labels, predictions, average='weighted')\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ],
   "id": "5d6a8d41cfa1c776",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main Script",
   "id": "855dcd082ae8e1f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function to train and evaluate Graph Convolutional Network (GCN) models\n",
    "    with different graph normalization techniques, visualize training metrics, and perform\n",
    "    embedding analysis through PCA.\n",
    "\n",
    "    Assumes the presence of a GCN model class, data loader preparation functions, and\n",
    "    various normalization technique functions defined outside this script.\n",
    "    \"\"\"\n",
    "    # Configuration parameters\n",
    "    num_samples_per_type = 1000  # Number of samples per class/type\n",
    "    num_epochs = 200  # Number of training epochs\n",
    "    # Dictionary mapping normalization technique names to their corresponding functions\n",
    "\n",
    "    # Lists for storing evaluation metrics and model information\n",
    "    metric_values = [[] for _ in range(4)]  # Lists to store Accuracy, Precision, Recall, F1 Score\n",
    "    train_losses = []  # Training loss values for each normalization technique\n",
    "\n",
    "    # Set the computation device (GPU or CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(f\"\\nTraining model\")\n",
    "    # Prepare data loaders\n",
    "    train_loader, test_loader = prepare_data_loaders(num_samples_per_type, batch_size=50)\n",
    "    print(f\"DataLoader batch size: {train_loader.batch_size}\")\n",
    "\n",
    "    # Initialize the GCN model, optimizer, and loss criterion\n",
    "    model = IMOTION_CNN().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train the model\n",
    "    train_losses = train_model(model, train_loader, optimizer, criterion, epochs=num_epochs, device=device)\n",
    "\n",
    "    # Evaluate the model's performance\n",
    "    accuracy, precision, recall, f1 = evaluate_model(model, test_loader, device=device)\n",
    "    # Store the evaluation metrics\n",
    "    metric_values[0].append(accuracy)\n",
    "    metric_values[1].append(precision)\n",
    "    metric_values[2].append(recall)\n",
    "    metric_values[3].append(f1)\n",
    "\n",
    "    # Output the evaluation results\n",
    "    print(f\"Results  - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Visualization of training losses and evaluation metrics for each normalization technique\n",
    "    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    plot_training_losses(train_losses)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "78d42560882cae31"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
