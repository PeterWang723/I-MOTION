{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# I-MOTION Model ",
   "id": "d55c8b69028c2f30"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T17:52:41.229984Z",
     "start_time": "2024-05-22T17:52:41.215804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "window_size = 600\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")"
   ],
   "id": "3a1093f4cb8ad685",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available. Using CPU.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Data",
   "id": "22bcf9d0d02583f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T17:58:28.006245Z",
     "start_time": "2024-05-22T17:58:11.813019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_x = np.loadtxt(\"./train/raw_data/Acc_x.txt\", delimiter=\" \")\n",
    "data_y = np.loadtxt(\"./train/raw_data/Acc_y.txt\", delimiter=\" \")\n",
    "data_z = np.loadtxt(\"./train/raw_data/Acc_z.txt\", delimiter=\" \")\n",
    "train_label = np.loadtxt(\"./train/train_label.txt\", delimiter=\" \")\n",
    "train_order = np.loadtxt(\"./train/train_order.txt\", dtype=float)\n",
    "acc_data = np.dstack((data_x, data_y, data_z, train_label))\n",
    "acc_data = acc_data[train_order.astype(np.int_).flatten() - 1, :, :]\n",
    "print(acc_data.shape)\n",
    "window_data = np.empty((acc_data.shape[0] * (acc_data.shape[1]/window_size), window_size, 3), dtype=np.float32)\n",
    "windowed_labels = np.empty((10000, 1), dtype=np.float32)\n",
    "\n",
    "window_index = 0\n",
    "for frame_index in range(acc_data.shape[0]):\n",
    "    \n",
    "    frame_features = acc_data[frame_index, :, :3]\n",
    "    frame_labels = acc_data[frame_index, :, 3]\n",
    "    \n",
    "    for start in range(0, acc_data.shape[1], window_size):\n",
    "        end = start + window_size\n",
    "        window_data[window_index] = frame_features[start:end, :]\n",
    "        # Assuming all samples in a window have the same label, take the label of the first sample\n",
    "        windowed_labels[window_index] = frame_labels[start]\n",
    "        window_index += 1\n",
    "\n",
    "# Verify the shapes\n",
    "print(\"Features shape:\", window_data.shape)  # Should be (10000, 600, 3)\n",
    "print(\"Labels shape:\", windowed_labels.shape)  # Should be (10000, 1)\n",
    "    "
   ],
   "id": "69106f9c06a7d448",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16310, 6000, 4)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Helper Function",
   "id": "98082d55a584d3f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T13:25:10.807791Z",
     "start_time": "2024-05-21T13:25:10.804961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_padding(kernel_size):\n",
    "    return (kernel_size - 1) // 2\n",
    "\n",
    "def calculate_output_length(input_length, kernel_size, stride, padding):\n",
    "    return ((input_length + 2 * padding - kernel_size) // stride) + 1\n",
    "\n",
    "def calculate_pooling_padding(window_size, stride, input_size, output_size):\n",
    "    return ((output_size - 1) * stride - input_size + window_size) // 2"
   ],
   "id": "ab1afb017abc5998",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "104355b2c124d563"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def preprocess_accelerometer_data(accel_data, alpha=0.8, m=5):\n",
    "    # Initialize gravity and linear acceleration\n",
    "    gravity = np.zeros((1, 3))\n",
    "    linear_acceleration = np.zeros_like(accel_data)\n",
    "    \n",
    "    # Step 1: Remove Gravity\n",
    "    for i in range(accel_data.shape[0]):\n",
    "        gravity = alpha * gravity + (1 - alpha) * accel_data[i, :]\n",
    "        linear_acceleration[i, :] = accel_data[i, :] - gravity\n",
    "\n",
    "    # Step 2: Smooth Data\n",
    "    K = len(linear_acceleration.shape(0))\n",
    "    smoothed_data = np.zeros_like(linear_acceleration)\n",
    "    \n",
    "    for k in range(1, K + 1):  # k is 1-indexed in the mathematical formula\n",
    "        if k <= m // 2:\n",
    "            # Early data points: smaller window size that grows\n",
    "            smoothed_data[k - 1, :] = np.sum(linear_acceleration[:2 * k - 1, :], axis=0) / (2 * k - 1)\n",
    "        elif k > K - m // 2:\n",
    "            # Late data points: smaller window size that shrinks\n",
    "            smoothed_data[k - 1, :] = np.sum(linear_acceleration[2 * k - K - 1:, :]) / (2 * (K - k) + 1)\n",
    "        else:\n",
    "            # Middle data points: fixed window size\n",
    "            smoothed_data[k - 1, :] = np.sum(linear_acceleration[k - m // 2 - 1 : k + m // 2, :]) / m\n",
    "\n",
    "    # Step 3: Calculate Magnitude\n",
    "    magnitudes = np.sqrt(np.sum(smoothed_data**2, axis=1))\n",
    "\n",
    "    return magnitudes\n",
    "\n",
    "# Example usage\n",
    "# Assuming `accel_data` is a numpy array of shape (512, 3) representing x, y, z acceleration\n",
    "accel_data = np.random.rand(512, 3)  # Random data for demonstration purposes\n",
    "processed_data = preprocess_accelerometer_data(accel_data)\n",
    "\n",
    "print(processed_data.shape)  # Output the shape of the processed data\n"
   ],
   "id": "6c182e7b1d7a5ba5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Construction",
   "id": "490b76b9e6eefb4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class IMOTION_CNN(nn.Module):\n",
    "    def __init__(self, in_feature, in_channel, out_feature, pool_window_size, pool_stride_size, \n",
    "                 pool_padding, conv_filter_list, conv_kernel_list, conv_stride, full_connection_size):\n",
    "        super(IMOTION_CNN, self).__init__()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=pool_window_size, stride=pool_stride_size, padding=pool_padding)\n",
    "        assert len(conv_filter_list) == len(conv_kernel_list)\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=in_channel if i == 0 else conv_filter_list[i-1], \n",
    "                      out_channels=conv_filter_list[i], \n",
    "                      kernel_size=conv_kernel_list[i], stride=conv_stride)\n",
    "            for i in range(len(conv_filter_list))\n",
    "        ])\n",
    "        self.final_matrix_size = conv_filter_list[-1] * (in_feature / (2 ** len(conv_filter_list)))\n",
    "        self.fc1 = nn.Linear(self.final_matrix_size, full_connection_size) \n",
    "        self.fc2 = nn.Linear(full_connection_size, out_feature)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv in self.conv_layers:\n",
    "            x = self.pool(torch.relu(conv(x)))\n",
    "        x = x.view(-1, self.final_matrix_size)  # Flattening the tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "id": "c473c5560a7c999d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training & Evaluation",
   "id": "33cd9c130671e96c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:16:05.900219Z",
     "start_time": "2024-05-21T15:16:05.890099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, epochs=100, device='cpu'):\n",
    "    model.train()  # Set the model to training mode\n",
    "    loss_values = []  # Initialize a list to store the average loss per epoch\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0  # Track total loss for each epoch\n",
    "\n",
    "        for X, A, labels in train_loader:\n",
    "            # Move data to the specified device\n",
    "            X, A, labels = X.to(device), A.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Clear gradients for the next train step\n",
    "            output = model(X, A)  # Forward pass\n",
    "            loss = criterion(output, labels)  # Compute the loss\n",
    "            loss.backward()  # Backward pass to compute gradients\n",
    "            optimizer.step()  # Update model parameters\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate the loss\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)  # Calculate average loss\n",
    "        loss_values.append(avg_loss)  # Append average loss to list\n",
    "\n",
    "        # Print the average loss for the current epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "    return loss_values\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, device='cpu'):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a test dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model to be evaluated.\n",
    "        test_loader (torch.utils.data.DataLoader): DataLoader for the test data.\n",
    "        device (str, optional): The device to run the model on ('cpu' or 'cuda'). Defaults to 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the accuracy, precision, recall, and F1 score of the model on the test dataset.\n",
    "\n",
    "    This function performs a forward pass on the test dataset to obtain the model's predictions,\n",
    "    then calculates and returns various evaluation metrics including accuracy, precision, recall, and F1 score.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    true_labels = []  # List to store actual labels\n",
    "    predictions = []  # List to store model predictions\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for X, A, labels in test_loader:\n",
    "            # Move data to the specified device\n",
    "            X, A, labels = X.to(device), A.to(device), labels.to(device)\n",
    "\n",
    "            output = model(X, A)  # Forward pass\n",
    "            _, predicted = torch.max(output.data, 1)  # Get the index of the max log-probability\n",
    "\n",
    "            true_labels += labels.tolist()  # Append actual labels\n",
    "            predictions += predicted.tolist()  # Append predicted labels\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='weighted')\n",
    "    recall = recall_score(true_labels, predictions, average='weighted')\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ],
   "id": "5d6a8d41cfa1c776",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main Script",
   "id": "855dcd082ae8e1f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test",
   "id": "818814941a720817"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1037ebae3f77799f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
