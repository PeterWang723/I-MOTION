{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# I-MOTION Model ",
   "id": "d55c8b69028c2f30"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T14:59:37.246605Z",
     "start_time": "2024-05-23T14:59:35.839228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "window_size = 600\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")"
   ],
   "id": "3a1093f4cb8ad685",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available. Using CPU.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Data",
   "id": "22bcf9d0d02583f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T15:28:43.817987Z",
     "start_time": "2024-05-23T15:28:25.282475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_x = np.loadtxt(\"./train/raw_data/Acc_x.txt\", delimiter=\" \")\n",
    "data_y = np.loadtxt(\"./train/raw_data/Acc_y.txt\", delimiter=\" \")\n",
    "data_z = np.loadtxt(\"./train/raw_data/Acc_z.txt\", delimiter=\" \")\n",
    "train_label = np.loadtxt(\"./train/train_label.txt\", delimiter=\" \")\n",
    "train_order = np.loadtxt(\"./train/train_order.txt\", dtype=float)\n",
    "acc_data = np.dstack((data_x, data_y, data_z, train_label))\n",
    "acc_data = acc_data[train_order.astype(np.int_).flatten() - 1, :, :]\n",
    "window_data = np.empty((acc_data.shape[0] * (acc_data.shape[1]//window_size), window_size, 3), dtype=np.float32)\n",
    "windowed_labels = np.empty((acc_data.shape[0] * (acc_data.shape[1]//window_size), 1), dtype=np.float32)\n",
    "\n",
    "window_index = 0\n",
    "for frame_index in range(acc_data.shape[0]):\n",
    "    \n",
    "    frame_features = acc_data[frame_index, :, :3]\n",
    "    frame_labels = acc_data[frame_index, :, 3]\n",
    "    \n",
    "    for start in range(0, acc_data.shape[1], window_size):\n",
    "        end = start + window_size\n",
    "        window_data[window_index] = frame_features[start:end, :]\n",
    "        # Assuming all samples in a window have the same label, take the label of the first sample\n",
    "        windowed_labels[window_index] = frame_labels[start]\n",
    "        window_index += 1\n",
    "\n",
    "# Verify the shapes\n",
    "print(\"Features shape:\", window_data.shape)\n",
    "print(\"Labels shape:\", windowed_labels.shape)\n",
    "    "
   ],
   "id": "69106f9c06a7d448",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16310, 6000, 4)\n",
      "(163100, 600, 3)\n",
      "Features shape: [[[-2.36748   9.603528 -1.200628]\n",
      "  [-2.442098  9.238089 -1.462477]\n",
      "  [-2.774196  8.611417 -1.739763]\n",
      "  ...\n",
      "  [-1.525542  9.455682  0.303084]\n",
      "  [-1.469803  9.531071  0.279402]\n",
      "  [-1.421543  9.6702    0.285504]]\n",
      "\n",
      " [[-1.367722  9.750951  0.306458]\n",
      "  [-1.334744  9.776136  0.295097]\n",
      "  [-1.316464  9.764919  0.287304]\n",
      "  ...\n",
      "  [ 0.137368 12.029835 -2.38063 ]\n",
      "  [ 0.710383 13.127102 -2.832197]\n",
      "  [ 1.297946 13.61043  -2.181224]]\n",
      "\n",
      " [[ 2.160904 13.322191 -1.865706]\n",
      "  [ 2.389481 12.746821 -1.159406]\n",
      "  [ 2.072968 12.296709 -1.122953]\n",
      "  ...\n",
      "  [-0.124745  8.965409 -2.24249 ]\n",
      "  [-0.341386  8.807157 -2.135628]\n",
      "  [-0.476393  8.848304 -2.124934]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.465765  4.178371 -8.714756]\n",
      "  [ 1.023613  3.981262 -8.475474]\n",
      "  [ 1.223304  3.938922 -8.735217]\n",
      "  ...\n",
      "  [ 1.207551  3.81527  -8.77083 ]\n",
      "  [ 1.504728  3.894734 -8.909285]\n",
      "  [ 1.894138  3.928993 -9.014277]]\n",
      "\n",
      " [[ 1.815587  3.903015 -8.920031]\n",
      "  [ 1.397466  3.75651  -8.670224]\n",
      "  [ 1.29213   3.754843 -8.610285]\n",
      "  ...\n",
      "  [ 1.307542  3.87562  -8.709008]\n",
      "  [ 1.653765  3.970687 -8.718581]\n",
      "  [ 1.774773  3.983952 -8.747328]]\n",
      "\n",
      " [[ 1.589664  3.910515 -8.704244]\n",
      "  [ 1.334502  3.834049 -8.664086]\n",
      "  [ 1.463471  3.904275 -8.647857]\n",
      "  ...\n",
      "  [ 2.230459  5.705789 -7.106883]\n",
      "  [ 2.802514  5.92651  -6.944612]\n",
      "  [ 2.677414  6.121533 -6.832821]]]\n",
      "Labels shape: [[7.]\n",
      " [7.]\n",
      " [7.]\n",
      " ...\n",
      " [6.]\n",
      " [6.]\n",
      " [6.]]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Helper Function",
   "id": "98082d55a584d3f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T13:25:10.807791Z",
     "start_time": "2024-05-21T13:25:10.804961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_training_losses(train_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses)\n",
    "    plt.title('Training Loss Over Epochs', fontsize=16)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_metric_bar_charts(names, metric_values, metric_names):\n",
    "    \"\"\"\n",
    "    Creates bar charts for different evaluation metrics across various normalization techniques.\n",
    "\n",
    "    Args:\n",
    "        names (list of str): Names of the normalization techniques.\n",
    "        metric_values (list of lists): Each sublist contains the values of a metric for each normalization technique.\n",
    "        metric_names (list of str): Names of the metrics being plotted.\n",
    "\n",
    "    This function plots a bar chart for each provided metric, comparing the performance of different normalization techniques,\n",
    "    with y-axis limits dynamically adjusted to emphasize differences while capping at 1.\n",
    "    \"\"\"\n",
    "    num_metrics = len(metric_names)\n",
    "    num_rows = num_cols = int(math.ceil(math.sqrt(num_metrics)))\n",
    "\n",
    "    sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(12 * num_cols, 8 * num_rows))\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "    for i, metric_name in enumerate(metric_names):\n",
    "        ax = axes.flatten()[i] if num_metrics > 1 else axes\n",
    "\n",
    "        # Create DataFrame for seaborn\n",
    "        data = pd.DataFrame({\n",
    "            'Normalization Technique': np.repeat(names, len(metric_values[i])),\n",
    "            metric_name: np.concatenate([metric_values[i] for _ in names])\n",
    "        })\n",
    "\n",
    "        sns.barplot(x='Normalization Technique', y=metric_name, data=data, ax=ax, alpha=0.75)\n",
    "\n",
    "        # Dynamically adjust the y-axis limits\n",
    "        min_val = min(data[metric_name]) * 0.9  # Start slightly below the smallest value for better visibility\n",
    "        max_val = 1  # Ensuring the upper limit is 1\n",
    "        ax.set_ylim([min_val, max_val])\n",
    "\n",
    "        ax.set_xlabel('Normalization Technique', fontsize=14)\n",
    "        ax.set_ylabel(f'{metric_name} Value', fontsize=14)\n",
    "        ax.set_title(f'Comparison of {metric_name}', fontsize=16)\n",
    "        ax.tick_params(axis='x', rotation=45, labelsize=12)\n",
    "        ax.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "        # Add text labels above bars\n",
    "        for p, value in zip(ax.patches, np.concatenate([metric_values[i] for _ in names])):\n",
    "            ax.text(p.get_x() + p.get_width() / 2., p.get_height(), f'{value:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    # Hide unused subplots if the number of metrics is less than the number of subplot positions\n",
    "    for i in range(num_metrics, num_rows * num_cols):\n",
    "        if num_rows * num_cols == 1:\n",
    "            break\n",
    "        fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def calculate_padding(kernel_size):\n",
    "    return (kernel_size - 1) // 2\n",
    "\n",
    "def calculate_output_length(input_length, kernel_size, stride, padding):\n",
    "    return ((input_length + 2 * padding - kernel_size) // stride) + 1\n",
    "\n",
    "def calculate_pooling_padding(window_size, stride, input_size, output_size):\n",
    "    return ((output_size - 1) * stride - input_size + window_size) // 2"
   ],
   "id": "ab1afb017abc5998",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "104355b2c124d563"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def preprocess_accelerometer_data(accel_data, alpha=0.8, m=5):\n",
    "    # Initialize gravity and linear acceleration\n",
    "    gravity = np.zeros((1, 3))\n",
    "    linear_acceleration = np.zeros_like(accel_data)\n",
    "    \n",
    "    # Step 1: Remove Gravity\n",
    "    for i in range(accel_data.shape[0]):\n",
    "        gravity = alpha * gravity + (1 - alpha) * accel_data[i, :]\n",
    "        linear_acceleration[i, :] = accel_data[i, :] - gravity\n",
    "\n",
    "    # Step 2: Smooth Data\n",
    "    K = len(linear_acceleration.shape(0))\n",
    "    smoothed_data = np.zeros_like(linear_acceleration)\n",
    "    \n",
    "    for k in range(1, K + 1):  # k is 1-indexed in the mathematical formula\n",
    "        if k <= m // 2:\n",
    "            # Early data points: smaller window size that grows\n",
    "            smoothed_data[k - 1, :] = np.sum(linear_acceleration[:2 * k - 1, :], axis=0) / (2 * k - 1)\n",
    "        elif k > K - m // 2:\n",
    "            # Late data points: smaller window size that shrinks\n",
    "            smoothed_data[k - 1, :] = np.sum(linear_acceleration[2 * k - K - 1:, :]) / (2 * (K - k) + 1)\n",
    "        else:\n",
    "            # Middle data points: fixed window size\n",
    "            smoothed_data[k - 1, :] = np.sum(linear_acceleration[k - m // 2 - 1 : k + m // 2, :]) / m\n",
    "\n",
    "    # Step 3: Calculate Magnitude\n",
    "    magnitudes = np.sqrt(np.sum(smoothed_data**2, axis=1)).reshape(1,-1)\n",
    "\n",
    "    return magnitudes\n",
    "\n",
    "processed_data = np.empty((window_size, window_data.shape[0]))\n",
    "for i in range(window_data.shape[0]):\n",
    "    processed_data[i, :] = preprocess_accelerometer_data(window_data[i, :, :])\n",
    "\n",
    "print(processed_data.shape)  # Output the shape of the processed data\n"
   ],
   "id": "6c182e7b1d7a5ba5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Construction",
   "id": "490b76b9e6eefb4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class IMOTION_CNN(nn.Module):\n",
    "    def __init__(self, in_feature=0, in_channel=0, out_feature=0, pool_window_size=0, pool_stride_size=0, \n",
    "                 pool_padding=0, conv_filter_list=0, conv_kernel_list=0, conv_stride=0, full_connection_size=0):\n",
    "        super(IMOTION_CNN, self).__init__()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=pool_window_size, stride=pool_stride_size, padding=pool_padding)\n",
    "        assert len(conv_filter_list) == len(conv_kernel_list)\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=in_channel if i == 0 else conv_filter_list[i-1], \n",
    "                      out_channels=conv_filter_list[i], \n",
    "                      kernel_size=conv_kernel_list[i], stride=conv_stride)\n",
    "            for i in range(len(conv_filter_list))\n",
    "        ])\n",
    "        self.final_matrix_size = conv_filter_list[-1] * (in_feature / (2 ** len(conv_filter_list)))\n",
    "        self.fc1 = nn.Linear(self.final_matrix_size, full_connection_size) \n",
    "        self.fc2 = nn.Linear(full_connection_size, out_feature)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv in self.conv_layers:\n",
    "            x = self.pool(torch.relu(conv(x)))\n",
    "        x = x.view(-1, self.final_matrix_size)  # Flattening the tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "id": "c473c5560a7c999d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset",
   "id": "e45a94f47793432e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define a custom Dataset\n",
    "class AccDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)  # Convert features to PyTorch tensors\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)       # Convert labels to PyTorch tensors\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "def prepare_data_loaders(train_dataset, test_dataset, batch_size=10):\n",
    "\n",
    "    train_dataset = AccDataset(train_dataset)\n",
    "    test_dataset = AccDataset(test_dataset)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ],
   "id": "7d2807cea662af84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training & Evaluation",
   "id": "33cd9c130671e96c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:16:05.900219Z",
     "start_time": "2024-05-21T15:16:05.890099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, epochs=100, device='cpu'):\n",
    "    model.train()  # Set the model to training mode\n",
    "    loss_values = []  # Initialize a list to store the average loss per epoch\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0  # Track total loss for each epoch\n",
    "\n",
    "        for X, A, labels in train_loader:\n",
    "            # Move data to the specified device\n",
    "            X, A, labels = X.to(device), A.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Clear gradients for the next train step\n",
    "            output = model(X, A)  # Forward pass\n",
    "            loss = criterion(output, labels)  # Compute the loss\n",
    "            loss.backward()  # Backward pass to compute gradients\n",
    "            optimizer.step()  # Update model parameters\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate the loss\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)  # Calculate average loss\n",
    "        loss_values.append(avg_loss)  # Append average loss to list\n",
    "\n",
    "        # Print the average loss for the current epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "    return loss_values\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, device='cpu'):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a test dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model to be evaluated.\n",
    "        test_loader (torch.utils.data.DataLoader): DataLoader for the test data.\n",
    "        device (str, optional): The device to run the model on ('cpu' or 'cuda'). Defaults to 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the accuracy, precision, recall, and F1 score of the model on the test dataset.\n",
    "\n",
    "    This function performs a forward pass on the test dataset to obtain the model's predictions,\n",
    "    then calculates and returns various evaluation metrics including accuracy, precision, recall, and F1 score.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    true_labels = []  # List to store actual labels\n",
    "    predictions = []  # List to store model predictions\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for X, A, labels in test_loader:\n",
    "            # Move data to the specified device\n",
    "            X, A, labels = X.to(device), A.to(device), labels.to(device)\n",
    "\n",
    "            output = model(X, A)  # Forward pass\n",
    "            _, predicted = torch.max(output.data, 1)  # Get the index of the max log-probability\n",
    "\n",
    "            true_labels += labels.tolist()  # Append actual labels\n",
    "            predictions += predicted.tolist()  # Append predicted labels\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='weighted')\n",
    "    recall = recall_score(true_labels, predictions, average='weighted')\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ],
   "id": "5d6a8d41cfa1c776",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main Script",
   "id": "855dcd082ae8e1f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function to train and evaluate Graph Convolutional Network (GCN) models\n",
    "    with different graph normalization techniques, visualize training metrics, and perform\n",
    "    embedding analysis through PCA.\n",
    "\n",
    "    Assumes the presence of a GCN model class, data loader preparation functions, and\n",
    "    various normalization technique functions defined outside this script.\n",
    "    \"\"\"\n",
    "    # Configuration parameters\n",
    "    num_samples_per_type = 1000  # Number of samples per class/type\n",
    "    num_epochs = 200  # Number of training epochs\n",
    "    # Dictionary mapping normalization technique names to their corresponding functions\n",
    "\n",
    "    # Lists for storing evaluation metrics and model information\n",
    "    metric_values = [[] for _ in range(4)]  # Lists to store Accuracy, Precision, Recall, F1 Score\n",
    "    train_losses = []  # Training loss values for each normalization technique\n",
    "\n",
    "    # Set the computation device (GPU or CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(f\"\\nTraining model\")\n",
    "    # Prepare data loaders\n",
    "    train_loader, test_loader = prepare_data_loaders(num_samples_per_type, batch_size=50)\n",
    "    print(f\"DataLoader batch size: {train_loader.batch_size}\")\n",
    "\n",
    "    # Initialize the GCN model, optimizer, and loss criterion\n",
    "    model = IMOTION_CNN().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train the model\n",
    "    train_losses = train_model(model, train_loader, optimizer, criterion, epochs=num_epochs, device=device)\n",
    "\n",
    "    # Evaluate the model's performance\n",
    "    accuracy, precision, recall, f1 = evaluate_model(model, test_loader, device=device)\n",
    "    # Store the evaluation metrics\n",
    "    metric_values[0].append(accuracy)\n",
    "    metric_values[1].append(precision)\n",
    "    metric_values[2].append(recall)\n",
    "    metric_values[3].append(f1)\n",
    "\n",
    "    # Output the evaluation results\n",
    "    print(f\"Results  - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Visualization of training losses and evaluation metrics for each normalization technique\n",
    "    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    plot_training_losses(train_losses)\n",
    "    plot_metric_bar_charts(normalization_names, metric_values, metric_names)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "78d42560882cae31"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
